\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[greek,english]{babel}
\usepackage{alphabeta}
\usepackage{amsmath}

\begin{document}
\title{Ex. 2 \& 4}
\maketitle

\section*{EX. 2}

Μας δίνεται η ακολουθία i.i.d. ζευγών τυχαίων μεταβλητών $(X_{1},Y_{1}),...,(X_{n},Y_{n}),...$ \\
με αναμενόμενη τιμή $E(X,Y)=(μ,ν)$ και $δ=Cov(X_{i},Y_{i})$ για κάθε $ n\in\lbrace 2,3,...\rbrace$
\\
\\
\\
\\
Ορίζουμε τον εκτιμητή της δ : $W_{n}=\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\mu)(Y_{i}-\nu)$ \\\\
\begin{center}
$$
\sum_{i=1}^{n}(X_{i}-\mu)(Y_{i}-\nu)=\sum_{i=1}^{n}X_{i}Y_{i}-\nu\sum_{i=1}^{n}X_{i}
-\mu\sum_{i=1}^{n}Y_{i}+n\mu\nu 
$$
$$
=\sum_{i=1}^{n}X_{i}Y_{i}-2n\mu\nu+n\mu\nu=\sum_{i=1}^{n}X_{i}Y_{i}-n\mu\nu 
$$\label{1} 


\end{center}

Αρα για την αναμενόμενη τιμή έχουμε:
\begin{center}
$$
E(W_{n})=\dfrac{1}{n}(\sum_{i=1}^{n}X_{i}Y_{i}-n\mu\nu)
$$
$$
=\dfrac{1}{n}(\sum_{i=1}^{n}E(X_{i}Y_{i})-nE(X_{i})E(Y_{i}))
$$
$$
=\dfrac{1}{n}(nE(X_{i}Y_{i})-nE(X_{i})E(Y_{i}))
$$
$$
=Cov(X_{i},Y_{i})=\delta
$$
\end{center}
Διότι $E(X_{i}Y_{i})=Cov(X_{i},Y_{i})+E(X_{i})E(Y_{i})$
\\
\\
-Συνεπώς ο $W_{n}$ είναι \emph{\textbf{unbiased estimator}} της δ.

%a) Sn unbiasedness


\paragraph{}
Επιπλέον ορίζουμε τον εκτιμητή της δ:
$$
S_{n}=\dfrac{1}{n-1}\sum_{i=1}^{n}[X_{i}-(\dfrac{1}{n}\sum_{i=1}^{n}X_{i})][Y_{i}-(\dfrac{1}{n}\sum_{i=1}^{n}Y_{i})]
$$
και θέτουμε \emph{$M(X)=\dfrac{1}{n}\sum_{i=1}^{n}X_{i}$} και \emph{$M(Y)=\dfrac{1}{n}\sum_{i=1}^{n}Y_{i}$}
\\
Παίρνουμε το άθροισμα:

$$
\sum_{i=1}^{n}(X_{i}-M(X))(Y_{i}-M(Y))=\sum_{i=1}^{n}X_{i}Y_{i}-M(Y)\sum_{i=1}^{n}X_{i}
-M(X)\sum_{i=1}^{n}Y_{i}+nM(X)M(Y)
$$
$$
=\sum_{i=1}^{n}X_{i}Y_{i}-2nM(X)M(Y)+nM(X)M(Y)=\sum_{i=1}^{n}X_{i}Y_{i}-nM(X)M(Y) 
$$
Η αναμενόμενη τιμή του $S_{n}$ είναι:\\
$$
Ε(S_{n})=\dfrac{1}{n-1}Ε(\sum_{i=1}^{n}X_{i}Y_{i}-nM(X)M(Y))
$$
$$
=\dfrac{1}{n-1}(\sum_{i=1}^{n}Ε(X_{i}Y_{i})-nE(M(X)M(Y)))=\dfrac{n}{n-1}[Ε(X_{i}Y_{i})-E(M(X)M(Y))]
$$
\\
Ισχύει οτι:

\begin{align*}
E(X_{i}Y_{i}) &= Cov(X_{i},Y_{i})+E(X_{i})E(Y_{i})=\delta+\mu\nu \\
E(M(X)M(Y)) &= Cov(M(X),M(Y))+E(M(X))E(M(Y))=\dfrac{\delta}{n}+\mu\nu
\end{align*}

Επομένως:
\begin{align*}
Ε(S_{n})= \dfrac{1}{n-1}(n\delta+n\mu\nu-n\dfrac{\delta}{n}+n\mu\nu)=\dfrac{n-1}{n-1}\delta=\delta
\end{align*}
-Συνεπώς ο $S_{n}$ είναι \emph{\textbf{unbiased estimator}} της δ.

\paragraph{b)}
Μελτάμε την συνέπεια των παραπάνω ετκτιμητών.\\
Για τον $W_{n}$ αρκεί να δείξουμε ότι:\\

$Var(W_{n})=\dfrac{1}{n}(\delta_{2}-\delta^{2})$ for every $ n\in N_{+}$ \\
όπου:
\begin{align*}
Cov(X_{i},Y_{i}) &= \delta \\
E[(X-\mu)^{2}(Y-\nu)^{2}] &=\delta_{2}
\end{align*}
\begin{align*}
Var((X-\mu)(Y-\nu)&=E[(X-\mu)^{2}(Y-\nu)^{2}]- (E[(X-\mu)(Y-\nu)])^{2}\\
 &=\delta_{2}-\delta^{2}
\end{align*}
\\
\\
Αρα για το $W_{n}$ έχουμε:\\ $Var(W_{n})=\dfrac{1}{n^{2}}\sum_{i=1}^{n}Var((X-\mu)(Y-\nu))=\dfrac{n}{n^{2}}(\delta_{2}-\delta^{2})=\dfrac{1}{n}(\delta_{2}-\delta^{2})$
\\
\\
-Δηλαδή η ακολουθία του $W_{n}$ είναι \textbf{συνεπής}.\\
\paragraph{}
Αντιστοιχα για το $S_{n}$ αρκεί να δείξουμε οτι :\\
$Var(S_{n})=\dfrac{1}{n}(\delta_{2}+\dfrac{\sigma^{2}\tau^{2}}{n-1}-\dfrac{n-2}{n-1}\delta^{2})$ for every $ n\in N_{+}$ \\\\

Tο $S_{n}$ μπορεί να γραφεί και ως:
\begin{align*}
S_{n}=\dfrac{1}{2n(n-1)}\sum_{i=1}^{n}\sum_{j=1}^{n}(X_{i}-X_{j})(Y_{i}-Y_{j})
\end{align*}
Άρα:
\begin{align*}
Var(S_{n})=\dfrac{1}{4n^{2}(n-1)^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{k=1}^{n}\sum_{l=1}^{n}(X_{i}-X_{j})(Y_{i}-Y_{j})(X_{k}-X_{l})(Y_{k}-Y_{l})
\end{align*}
Για τις διαφορες τιμές των $i,j,k,l$ έχουμε:\\
1)$Cov((X_{i}-X_{j})(Y_{i}-Y_{j})(X_{k}-X_{l})(Y_{k}-Y_{l}))=0$~ για $i=j$και $k=l$.
\\ Υπάρχουν $n^{2}(2n-1)$ τέτοιες περιπτώσεις.\\\\
2)$Cov((X_{i}-X_{j})(Y_{i}-Y_{j})(X_{k}-X_{l})(Y_{k}-Y_{l}))=0$~ για $i,j,k,l$ διαφορετικά\\ μεταξύ τους.
\\ Υπάρχουν $n(n-1)(n-2)(n-3)$ τέτοιες περιπτώσεις.\\\\
3)$Cov((X_{i}-X_{j})(Y_{i}-Y_{j})(X_{k}-X_{l})(Y_{k}-Y_{l}))=2\delta_{2}+2\sigma^{2}\tau^{2}$ ~για $i\neq j$ και $\lbrace k,l\rbrace=\lbrace i,j\rbrace$
\\ Υπάρχουν $2n(n-1)$ τέτοιες περιπτώσεις.\\\\
4)$Cov((X_{i}-X_{j})(Y_{i}-Y_{j})(X_{k}-X_{l})(Y_{k}-Y_{l}))=\delta_{2}-\delta^{2}$ ~για $i\neq j,k \neq l$ και $\vert\lbrace k,l\rbrace\cap\lbrace i,j\rbrace\vert=1$
\\ Υπάρχουν $4n(n-1)(n-2)$ τέτοιες περιπτώσεις.\\

Άρα αντικαθιστώντας καταλήγουμε ότι:
\begin{align*}
Var(S_{n}) &=\dfrac{1}{4n^{2}(n-1)^{2}}(2n(n-1)(2\delta_{2}+2\sigma^{2}\tau^{2})+4n(n-1)(n-2)(2\delta_{2}-\delta^{2})) \\
&=\dfrac{1}{n(n-1)}(\delta_{2}+\sigma^{2}\tau^{2})+\dfrac{1}{n}\dfrac{n-2}{n-1}(\delta_{2}-\delta^{2}) \\
&=\dfrac{1}{n}(\delta_{2}+\dfrac{\sigma^{2}\tau^{2}}{n-1}-\dfrac{n-2}{n-1}\delta^{2})
\end{align*}
\\

\paragraph{c)}
Μελετάμε την αποδοτικότητα των $W_{n}$ και $S_{n}$ παίρνωντας την διαφορά τους:\\
\begin{align*}
Var(S_{n})-Var(W_{n})&= \dfrac{1}{n}(\delta_{2}+\dfrac{\sigma^{2}\tau^{2}}{n-1}-\dfrac{n-2}{n-1}\delta^{2}-\delta_{2}+\delta^{2}) \\
&=\dfrac{1}{n(n-1)}({\sigma^{2}\tau^{2}}+\delta^{2})>0
\end{align*}
Για κάθε n ισχύει $Var(S_{n})>Var(W_{n})$ και συνεπώς ο $W_{n}$ είναι αποδοτικότερος εκτιμητής της δ.

\paragraph{d)}
Για την ασυμπτωτική σχετική απόδοση των $W_{n} ,S_{n}$ έχουμε:\\

\begin{align*}
\lim_{n \to \infty}Eff(W_{n} ,S_{n}) &=\lim_{n \to \infty}\dfrac{Var(S_{n})}{Var(W_{n})}=\lim_{n \to \infty}(\dfrac{\delta_{2}+\dfrac{\sigma^{2}\tau^{2}}{n-1}-\dfrac{n-2}{n-1}\delta^{2}}{\delta_{2}-\delta^{2}})=\dfrac{\delta_{2}-\delta^{2}}{\delta_{2}-\delta^{2}}=1
\end{align*}

\newpage
\section*{EX. 4}
Έστω ακολουθία i.i.d r.v.s $X_{1},...X_{n},...$ με ομοιόμορφη συνεχή κατανομή στο $(0,\theta)$ \\
Προτείνονται οι εξής εκτιμητές για την παράμετρο θ :

\begin{align*}
\theta_{n}=\dfrac{2}{n}\sum_{i=1}^{n} X_{i} ~~~~and~~~~ \zeta_{n}=(n+1)min\lbrace X_{1},...X_{n} \rbrace
\end{align*}
\\
Για τις τυχαίες μεταβλητές έχουμε την συνάρτηςη πυκνότητας πιθανότητας :
\\
\begin{align*}
f_{x}(x ; \theta)=\dfrac{1}{\theta}1(x \in (0,\theta))
\end{align*}
και
\begin{align*}
E(X_{i})=\int_{-\infty}^{+\infty} xf_{x}(x ; \theta)dx=\int_{0}^{\theta}\dfrac{x}{\theta}dx=\dfrac{\theta^{2}}{2\theta}=\dfrac{\theta}{2}
\end{align*}
\\

Αρχικά θα μελετήσουμε τον εκτιμητή $\theta_{n}$ :
\*
\begin{align*}
E(\theta_{n})=E(\dfrac{2}{n}\sum_{i=1}^{n} X_{i})=\dfrac{2}{n}\sum_{i=1}^{n}E( X_{i})=\dfrac{2}{n}n\dfrac{\theta}{2}=\theta
\end{align*}
Αρα ο $\theta_{n}$ είναι \textbf{unbiased }εκτιμητής της θ
\*
Μελετάμε και την συνέπεια του $\theta_{n}$ 
\\
\begin{align*}
E((\theta_{n}-\theta)^{2})=Var(\theta_{n}) &=Var(\dfrac{2}{n}\sum_{i=1}^{n} X_{i})=\dfrac{4}{n^{2}}\sum_{i=1}^{n} Var(X_{i}) \\
&=\dfrac{4}{n^{2}}n\dfrac{1}{12}\theta^{2}=\dfrac{\theta^{2}}{3n}
\end{align*}
\*
και \[ \lim_{n \to \infty}(E((\theta_{n}-\theta)^{2}))=\lim_{n \to \infty}\dfrac{\theta^{2}}{3n}=0 \]
\* 
Συνεπώς η ακολουθία του $\theta_{n}$ είναι μεσοτετραγωνικά \textbf{συνεπής}.
\\
\\
\paragraph*{}
Για τον εκτιμητή $\zeta_{n}=(n+1)min\lbrace X_{1},...X_{n} \rbrace$ 
\\
\\
Έστω η $ Y=min\lbrace X_{1},...X_{n} \rbrace $ 
\\
Παίρνουμε απο την κατανομή του Y την συνάρτηση πυκνότητας πιθανότητας.


$F(y)=P(Y\leq y)=1-P(Y>y)=1-P(min\lbrace X_{1},...X_{n} \rbrace > y)$
\\
\\
Όμως έχουμε $min\lbrace X_{1},...X_{n} \rbrace > y$ μόνο όταν $X_{i}>y ~ \forall i$ \\
και εφόσον έχουμε i.i.d. rvs:
\begin{align*}
F(y)=1-P(X_{1}>y)P(X_{2}>y)...P(X_{n}>y)=1-P(X_{i}>y)^{n}
\end{align*}
\\
Τα $X_{i}$ έχουν uniform distribution στο $(0,\theta)$ και συνεπώς:\\
\begin{align*}
F(y)=\left\{
\begin{array}{ll}
      1-\left(\dfrac{\theta-y}{\theta}\right)^{n} & y\in (0,\theta) \\
      0 & y<0\\
      1 & y>\theta\\
\end{array} 
\right. 
\end{align*}
και παραγωγίζοντας:\\
\begin{align*}
f(y)=\left\{
\begin{array}{ll}
      \dfrac{n}{\theta}\left(\dfrac{\theta-y}{\theta}\right)^{n-1} & y\in (0,\theta) \\
      0 & otherwise\\      
\end{array} 
\right. 
\end{align*}
\\
Τώρα μπορούμε να παρουμε την αναμενόμενη τιμη της Y \\
\begin{align*}
E(Y)=\int_{-\infty}^{\infty}yf(y)dy=\int_{0}^{\theta}\dfrac{ny}{\theta}\left(\dfrac{\theta-y}{\theta}\right)^{n-1}dy=\dfrac{\theta}{n+1}
\end{align*}
\\
Αρα ο εκτιμητής μας είναι \textbf{unbiased} αφού έχει:
\[ E(\zeta_{n})=E((n+1)Y)=(n+1)E(Y)=(n+1)\dfrac{\theta}{n+1}=\theta\]
\\
Μελετάμε και την συνέπεια του $\zeta_{n}$ . Έτσι παίρνουμε:
\begin{align*}
E((\zeta_{n}-\theta)^{2})=Var(\zeta_{n})=Var((n+1)Y)=(n+1)^{2}Var(Y)
\end{align*}
\\
Υπολογίζω την Var(Y) απο την CDF της και έχω:
\begin{align*}
Var(Y)=2\int_{0}^{\infty}y(1-F(y))dy-\left(\int_{0}^{\infty}(1-F(y))dy \right)^{2}
\end{align*}
\begin{align*}
I_{1}=\int_{0}^{\infty}y(1-F(y))dy=\int_{0}^{\theta}y\left(\dfrac{\theta-y}{\theta}\right)^{n}dy=\dfrac{\theta^{2}}{n^{2}+3n+2}
\end{align*}
\begin{align*}
I_{2}=\int_{0}^{\infty}(1-F(y))dy=\int_{0}^{\theta}\left(\dfrac{\theta-y}{\theta}\right)^{n}dy=0+\dfrac{\theta}{n+1}
\end{align*}
\\
Άρα έχουμε: 
\begin{align*}
Var(\zeta_{n}) &=(n+1)^{2}Var(Y)=(n+1)^{2}(2I_{1}-I_{2}^{2}) \\
&=(n+1)^{2}\left(\dfrac{2\theta^{2}}{n^{2}+3n+2}-\dfrac{\theta^{2}}{(n+1)^{2}}\right) \\
&=\dfrac{n+1}{n+2}2\theta^{2}-\theta^{2}\\
&=\dfrac{n}{n+1}\theta^{2}
\end{align*}
\begin{align*}
\lim_{n \to \infty}Var(\zeta_{n})=\lim_{n \to \infty}\dfrac{n}{n+1}\theta^{2}=\theta^{2}
\end{align*}
Συνεπώς η ακολουθία του $\zeta_{n}$ δεν είναι \textbf{συνεπής}.
\\
Τέλος οσο αναφρά την αποδοτικότητα των 2 εκτιμητών έχουμε:
\\
\begin{align*}
Var(\zeta_{n})-Var(\theta_{n})=\theta^{2}\left(\dfrac{n}{n+1}- \dfrac{1}{3n}\right)=\theta^{2}\left(\dfrac{3n^{2}-n-1}{3n(n+1)}\right)>0 ~~for~n>1/6 (1 + \sqrt{13})
\end{align*}
\[ Ομως~ n \in N_{+} ~Αρα~ισχυει~για~κάθε~ n>0 \]
\\
Συνεπώς ο $\theta_{n}$ είναι \textbf{αποδοτικότερος} εκτιμητής της παραμέτρου $\theta$ απο την $\zeta_{n}$
\end{document}


